{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 接下来我们就用rasa来完成上面的建模过程，完成完整的KBQA过程\n",
    "\n",
    "* 构建我们的NLU训练语料\n",
    "chatITO的使用：\n",
    "https://rodrigopivi.github.io/Chatito/\n",
    "\n",
    "##### chatito的主要原理从下面的语句看，原理很简单，定义我们问句的结构，定义意图的名字，然后句子的结构拆解为及格模板，及格模板再通过枚举的方式从@[entity_name]循环提取生成不同的句子\n",
    " ```json\n",
    "%[request_professor_diagnosis]('training': '2000', 'testing': '150')  #指定生成多少个样本\n",
    "    *[30%] ~[name] ~[action] @[disease]~[professor] #百分比表示当前这个模板生成的样本在总样本占的比例\n",
    "    *[20%] ~[name] ~[action] @[drug]~[professor]\n",
    "    *[10%] ~[action] @[symptom]\n",
    "    *[20%] ~[name] ~[action] @[symptom]~[professor]\n",
    "    *[20%] ~[action] ~[professor]\n",
    "@[disease] #定义模板每个模块的字典\n",
    "    百日咳\n",
    "    苯中毒\n",
    " ```\n",
    "#### 我们的KBQA案例中用到的chatito语句部分如下：\n",
    " ```json\n",
    "%[search_treat]('training': '3000', 'testing': '300')\n",
    "    *[50%] ~[get?]@[disease]~[时?]~[的?]~[treat_qwd?]~[qwd?]\n",
    "    *[10%] ~[treat_qwd?]@[disease]~[时?]~[的?]~[qwd?]\n",
    "    *[15%] @[disease]~[是什么]~[病?]~[qwd?]\n",
    "    *[15%] ~[什么是]@[disease]~[qwd?]\n",
    "    *[10%] @[disease]\n",
    "\n",
    "%[search_food]('training': '1500', 'testing': '150')\n",
    "    *[30%] ~[get?]@[disease]~[时?]~[的?]~[food_qwd]~[qwd?]\n",
    "    *[30%] ~[get?]@[disease]~[时?]~[的?]~[food_qwd]~[qwd?]\n",
    "    *[40%] ~[get?]@[disease]~[时?]~[的?]~[可以吃?]~[什么]~[food_qwd]~[qwd?]\n",
    "\n",
    "%[search_symptom]('training': '1500', 'testing': '150')\n",
    "    *[33%] ~[get?]@[disease]~[时?]~[会?]~[不会?]~[有?]~[什么?]~[symptom_qwd]~[qwd?]\n",
    "    *[33%] ~[get?]@[disease]~[的?]~[symptom_qwd]~[qwd?]\n",
    "    *[33%] ~[get?]@[disease]~[时?]~[有?]~[什么?]~[symptom_qwd]~[qwd?]\n",
    "\n",
    "%[search_cause]('training': '1500', 'testing': '150')\n",
    "    *[60%] ~[why_qwd]~[get?]@[disease]~[qwd?]\n",
    "    *[40%] ~[get?]@[disease]~[的?]~[cause_qwd]~[qwd?]\n",
    "\n",
    "%[search_neopathy]('training': '1500', 'testing': '150')\n",
    "    *[50%] ~[get?]@[disease]~[会?]~[不会?]~[有?]~[什么?]~[neopathy_qwd]~[qwd?]\n",
    "    *[50%] ~[get?]@[disease]~[的?]~[neopathy_qwd]~[有?]~[什么?]~[qwd?]\n",
    "\n",
    "%[search_drug]('training': '1500', 'testing': '150')\n",
    "    *[60%] ~[get?]@[disease]~[该?]~[吃?]~[什么?]~[drug_qwd]~[qwd?]\n",
    "    *[40%] ~[get?]@[disease]~[时?]~[的?]~[drug_qwd]~[qwd?]\n",
    "\n",
    "%[search_prevention]('training': '1500', 'testing': '150')\n",
    "    *[100%] ~[prevention_qwd]~[get?]@[disease]~[qwd?]\n",
    "\n",
    "%[search_drug_func]('training': '1000', 'testing': '150')\n",
    "    *[100%] @[drug]~[可以?]~[treat_what_qwd?]~[qwd?]\n",
    "\n",
    "%[search_disease_treat_time]('training': '1000', 'testing': '150')\n",
    "    *[70%] @[disease]~[要?]~[治?]~[treat_time_qwd]~[能好?]~[qwd?]\n",
    "    *[30%] ~[要?]~[治?]~[treat_time_qwd]@[disease]~[能好?]~[qwd?]\n",
    "\n",
    "%[search_easy_get]('training': '1000', 'testing': '150')\n",
    "    *[100%] ~[easy_get_qwd]@[disease]~[qwd?]\n",
    "\n",
    "%[search_disease_dept]('training': '1000', 'testing': '150')\n",
    "    *[100%] @[disease]~[dept_qwd]~[qwd?]\n",
    "\n",
    "~[qwd]\n",
    "    呢\n",
    "    吗\n",
    "    呀\n",
    "    好呢\n",
    "    吖\n",
    "    啊\n",
    "    阿\n",
    "\n",
    "~[get]\n",
    "    得\n",
    "    得了\n",
    "    生了\n",
    "    生\n",
    "    患\n",
    "    感染\n",
    "    染上\n",
    "    得上\n",
    "\n",
    "~[food_qwd]\n",
    "    饮食\n",
    "    饮用\n",
    "    吃\n",
    "    食\n",
    "    伙食\n",
    "    膳食\n",
    "    喝\n",
    "    菜\n",
    "    忌口\n",
    "    补品\n",
    "    保健品\n",
    "    食谱\n",
    "    菜谱\n",
    "    食用\n",
    "    食物\n",
    "    补品\n",
    "\n",
    "~[symptom_qwd]\n",
    "    症状\n",
    "    表征\n",
    "    现象\n",
    "    症候\n",
    "    表现\n",
    "\n",
    "~[cause_qwd]\n",
    "    原因\n",
    "    成因\n",
    "\n",
    "~[why_qwd]\n",
    "    为什么\n",
    "    怎么会\n",
    "    怎样才\n",
    "    咋样才\n",
    "    怎样会\n",
    "    如何会\n",
    "    为啥\n",
    "    为何\n",
    "    如何才会\n",
    "    怎么才会\n",
    "    会导致\n",
    "    会造成\n",
    "\n",
    "~[neopathy_qwd]\n",
    "    并发症\n",
    "    并发\n",
    "    一起发生\n",
    "    一并发生\n",
    "    一起出现\n",
    "    一并出现\n",
    "    一同发生\n",
    "    一同出现\n",
    "    伴随发生\n",
    "    伴随\n",
    "    共现\n",
    "\n",
    "~[drug_qwd]\n",
    "    药\n",
    "    药品\n",
    "    用药\n",
    "    胶囊\n",
    "    口服液\n",
    "    炎片\n",
    "\n",
    "~[prevention_qwd]\n",
    "    预防\n",
    "    防范\n",
    "    抵制\n",
    "    抵御\n",
    "    躲开\n",
    "    躲掉\n",
    "    绕开\n",
    "    怎样才能不\n",
    "    怎么才能不\n",
    "    咋样才能不\n",
    "    咋才能不\n",
    "    如何才能不\n",
    "    怎样才不\n",
    "    怎么才不\n",
    "    怎么才可不\n",
    "    咋样才可不\n",
    "    咋才可不\n",
    "    如何可不\n",
    "\n",
    "~[treat_time_qwd]\n",
    "    周期\n",
    "    多久\n",
    "    多长时间\n",
    "    多少时间\n",
    "    几天\n",
    "    几年\n",
    "    多少天\n",
    "    多少小时\n",
    "    几个小时\n",
    "    多少年\n",
    "\n",
    "~[treat_qwd]\n",
    "    怎么治疗\n",
    "    如何医治\n",
    "    怎么医治\n",
    "    怎么治\n",
    "    怎么医\n",
    "    如何治\n",
    "    医治方式\n",
    "    疗法\n",
    "    咋治\n",
    "    怎么办\n",
    "    咋办\n",
    "    咋治\n",
    "\n",
    "~[treat_prob_qwd]\n",
    "    多大概率能治好\n",
    "    多大几率能治好\n",
    "    治好希望大么\n",
    "    几率\n",
    "    几成\n",
    "    比例\n",
    "    可能性\n",
    "    能治\n",
    "    可治\n",
    "    可以治\n",
    "    可以医\n",
    "\n",
    "~[easy_get_qwd]\n",
    "    易感人群\n",
    "    容易感染\n",
    "    易发人群\n",
    "    什么人\n",
    "    哪些人\n",
    " ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 通过上面的工具，我们可以得到满足rasa训练要求的NLU训练格式：\n",
    "通用例子的组成主要包括三个部分：text，intent，entities。前两个是字符串，最后一个是数组。\n",
    "* text是用户消息，必填\n",
    "* intent是text对应的意图，选填\n",
    "* entities是text中需要被标识出来的部分，选填\n",
    "* Entities带有开始和结束值，组成python格式的范围选择，比如，text=\"show me chinese restaurants\"，那么text[8:15]=='chinese'。实体可以跨单词，实际上，value属性并不直接与子串相关。那样可以映射同义词，或误拼写的词，到相同的一个value。\n",
    "> ```shell\n",
    "> ## intent:restaurant_search\n",
    "> - show me [chinese](cuisine) restaurants\n",
    "> ```\n",
    "\n",
    " ```json\n",
    "  common_examples\":\n",
    "  [{\"text\":\"怎么医Goodpasture综合征时呢\",\"intent\":\"search_treat\",\"entities\":  \n",
    "  [{\"end\":17,\"entity\":\"disease\",\"start\":3,\"value\":\"Goodpasture综合征\"}]},\n",
    "  {\"text\":\"生了肺炎杆菌肺炎时医治方式吖\",\"intent\":\"search_treat\",\"entities\":\n",
    "  [{\"end\":8,\"entity\":\"disease\",\"start\":2,\"value\":\"肺炎杆菌肺炎\"}]},\n",
    "  {\"text\":\"患肺炎时的怎么医啊\",\"intent\":\"search_treat\",\"entities\":[{\"end\":3,\"entity\":\"disease\",\"start\":1,\"value\":\"肺炎\"}]},\n",
    "  \n",
    "  ```\n",
    "  \n",
    " ##### 可以看到生成的数据时json格式，每一个text，有两个指标去衡量，一个时意图，一个时entity，entity时采用双指针的标注方式，即记录实体在text的开始和结束index即可\n",
    " \n",
    " * 另外需要注意的是，在rasa1.0中，NLU的训练数据支持json和md格式，用chatito生成的是json格式，rasa也提供了格式的转换语法：\n",
    " ```json\n",
    " \n",
    " usage: rasa data convert nlu [-h] [-v] [-vv] [--quiet] --data DATA --out OUT\n",
    "                             [-l LANGUAGE] -f {json,md}\n",
    "\n",
    "optional arguments:\n",
    "  -h, --help            show this help message and exit\n",
    "  --data DATA           Path to the file or directory containing Rasa NLU\n",
    "                        data. (default: None)\n",
    "  --out OUT             File where to save training data in Rasa format.\n",
    "                        (default: None)\n",
    "  -l LANGUAGE, --language LANGUAGE\n",
    "                        Language of data. (default: en)\n",
    "  -f {json,md}, --format {json,md}\n",
    "                        Output format the training data should be converted\n",
    "                        into. (default: None)\n",
    "\n",
    "Python Logging Options:\n",
    "  -v, --verbose         Be verbose. Sets logging level to INFO. (default:\n",
    "                        None)\n",
    "  -vv, --debug          Print lots of debugging statements. Sets logging level\n",
    "                        to DEBUG. (default: None)\n",
    "  --quiet               Be quiet! Sets logging level to WARNING. (default:\n",
    "                        None)\n",
    "  ```\n",
    "> ```shell\n",
    "> rasa data convert nlu --data M3-training_dataset_1564317234.json --out M3-training_dataset_1564317234.md -f md\n",
    "> ```\n",
    "\n",
    "#### 对于md格式：\n",
    "Markdown是用户阅读和书写Rasa NLU最方便的格式。使用无序列表的格式，如-,*,+。例子通过意图进行组合，实体和实体的名字通过markdown的链接形式给出，如[entity](entity name)\n",
    "```json\n",
    "## intent:check_balance\n",
    "- what is my balance <!-- no entity -->\n",
    "- how much do I have on my [savings](source_account) <!-- entity \"source_account\" has value \"savings\" -->\n",
    "- how much do I have on my [savings account](source_account:savings) <!-- synonyms, method 1-->\n",
    "- Could I pay in [yen](currency)?  <!-- entity matched by lookup table -->\n",
    "​\n",
    "## intent:greet\n",
    "- hey\n",
    "- hello\n",
    "​\n",
    "## synonym:savings   <!-- synonyms, method 2 -->\n",
    "- pink pig\n",
    "​\n",
    "## regex:zipcode\n",
    "- [0-9]{5}\n",
    "​\n",
    "## lookup:currencies   <!-- lookup table list -->\n",
    "- Yen\n",
    "- USD\n",
    "- Euro\n",
    "​\n",
    "## lookup:additional_currencies  <!-- no list to specify lookup table file -->\n",
    "path/to/currencies.txt\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NLU数据中及格重要的组件：\n",
    "* lookuptable:\n",
    "查找表可以以list或txt文件（以newline分隔）的形式提供。当加载训练数据的时候，这些文件被用来生成大小写不敏感的正则化模式（基于正则化特征）。举个例子，货币列表中的名字很容易被当做实体挑选出来。\n",
    "lookuptable可以包括在训练数据中。外部提供的数据必须要以newline进行分隔。比如plates.txt可以包含：\n",
    " ```json\n",
    " tacos\n",
    "beef\n",
    "mapo tofu\n",
    "burrito\n",
    "lettuce wrap\n",
    " \n",
    "  ```\n",
    "训练文件中对应是：\n",
    " ```json\n",
    "## lookup:plates\n",
    "plates.txt \n",
    "  ```\n",
    "利用list的实现是：\n",
    " ```json\n",
    "## lookup:plates\n",
    "- tacos\n",
    "- beef\n",
    "- mapo tofu\n",
    "- burrito\n",
    "- lettuce wrap\n",
    "  ```\n",
    "* 对于我们的医疗数据，只需要将对应节点的一些名称放到lookuptable即可。\n",
    " ```json\n",
    " {\"rasa_nlu_data\":{\n",
    "  \"lookup_tables\":[\n",
    "            {\n",
    "                \"name\": \"disease\",\n",
    "                \"elements\": \"data/medical/lookup/Diseases.txt\"\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"symptom\",\n",
    "                \"elements\": \"data/medical/lookup/Symptoms.txt\"\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"dept\",\n",
    "                \"elements\": \"data/medical/lookup/Departments.txt\"\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"drug\",\n",
    "                \"elements\": \"data/medical/lookup/Drugs.txt\"\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"food\",\n",
    "                \"elements\": \"data/medical/lookup/Foods.txt\"\n",
    "            }\n",
    "   ]\n",
    " ```\n",
    "    \n",
    "* synonyms组件：\n",
    "同义词可以将提取出来的实体映射到相同的名字，如例子中将\"my savings account\"映射到\"savings\"。但是，这个仅仅发生在实体被提取出来之后，因此需要提供含有同义词表述的实例，使得Rasa能够通过学习将实体提取出来。需要注意的是，为了在训练数据中使用同义词，需要pipeline中包含EntitySynonmMapper组件.\n",
    " ```json\n",
    " \"entity_synonyms\":[],\"\n",
    " ```\n",
    "\n",
    "* regex组件：\n",
    "正则化表达式可以有助于支持意图分类和实体提取。举个例子，如果你的实体具有固定的结构（如邮编，或email地址），那么可以使用正则化表达式提取这些实体。如邮编可以用下面的表达式:\n",
    " ```json\n",
    " ## regex:zipcode\n",
    "- [0-9]{5}\n",
    "\n",
    "## regex:greet\n",
    "- hey[^\\\\s]*\n",
    "  ```\n",
    "这个名字并没有定义实体或意图，这个仅仅是用户可读的描述，用来帮助记住这些正则表达式的作用，同时是相关模式特征的标题。正如上面的例子中，可以使用正则化特征提高意图分类的性能。\n",
    "尽量的使用更加紧缩的匹配形式，如使用hey[^\\s]*替换hey.*。由于后一个表达式会匹配到更多无效的信息。\n",
    "正则化特征目前仅支持CRFEntityExtractor组件。其他的实体提取器，类似MitieEntityExtractor或SpacyEntityExtractor当前并不能使用正则化特征。当前，所有的意图分类器可以使用所有的正则化特征。\n",
    "注意：正则化特征并没有定义实体或意图。他们仅仅提供了模式用来帮助分类器识别实体和相关的意图。因此，你还是需要提供意图和实体的例子。\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 检验NLU等训练数据是否正确\n",
    "```\n",
    "python m rasa data validate\n",
    "```\n",
    "参数说明：\n",
    "```\n",
    "usage: rasa data validate [-h] [-v] [-vv] [--quiet] [--fail-on-warnings]\n",
    "                          [-d DOMAIN] [--data DATA]\n",
    "\n",
    "optional arguments:\n",
    "  -h, --help            show this help message and exit\n",
    "  --fail-on-warnings    Fail validation on warnings and errors. If omitted\n",
    "                        only errors will result in a non zero exit code.\n",
    "                        (default: False)\n",
    "  -d DOMAIN, --domain DOMAIN\n",
    "                        Domain specification (yml file). (default: domain.yml)\n",
    "  --data DATA           Path to the file or directory containing Rasa data.\n",
    "                        (default: data)\n",
    "\n",
    "Python Logging Options:\n",
    "  -v, --verbose         Be verbose. Sets logging level to INFO. (default:\n",
    "                        None)\n",
    "  -vv, --debug          Print lots of debugging statements. Sets logging level\n",
    "                        to DEBUG. (default: None)\n",
    "  --quiet               Be quiet! Sets logging level to WARNING. (default:\n",
    "                        None)\n",
    "```\n",
    "也可以自己编写脚本做检验：\n",
    "```\n",
    "import logging\n",
    "from rasa import utils\n",
    "from rasa.core.validator import Validator\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "utils.configure_colored_logging('DEBUG')\n",
    "\n",
    "validator = Validator.from_files(domain_file='domain.yml',\n",
    "                                 nlu_data='data/nlu_data.md',\n",
    "                                 stories='data/stories.md')\n",
    "\n",
    "validator.verify_all()\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### chatito不是很方便，那如何自己写一个文本生成器呢？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#下面只是一个小的参考，请根据我们的图谱设计，完成所以关系的查询NLU数据生成工具\n",
    "def nluData_generation_tool(intent_name,entityDic,relationDic,f):\n",
    "    f.write(\"## intent: \" + intent_name + '\\n')\n",
    "    for entity in entityDic:\n",
    "        for relation in relationDic:\n",
    "            #模板1\n",
    "            template1 = \"- 如果得了(\"+entity+\")[diseace]\"+\"会有怎么样呀？\"\n",
    "            #模板2\n",
    "            template2 = \"- 患上(\"+entity+\")[diseace]\"+\"可能有什么症状哦？\"\n",
    "            #模板3\n",
    "            template3 = \"- 如果得了(\"+entity+\")[diseace]\"+\"会有哪些症状呢？\"\n",
    "            #可以根据实际情况定义模板，继续生成\n",
    "            f.write(template1 + '\\n')\n",
    "            f.write(template2 + '\\n')\n",
    "            f.write(template3 + '\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 思考\n",
    "```json\n",
    "如果我们后面引入form表单，那这个数据生成器有有哪些变化？\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 接下来定义我们的Story\n",
    "Story主要是为对话管理模块服务的。 对话管理，即Dialog Management(DM)，它控制着人机对话的过程，是人机对话系统的重要组成部分。DM会根据NLU模块输出的语义表示执行对话状态的更新和追踪，并根据一定策略选择相应的候选动作。 简单来说，就是DM会根据对话历史信息，决定此刻对用户的反应，比如在任务驱动的多轮对话系统中，用户带着明确的目的如订餐、订票等，用户需求比较复杂，有很多限制条件，可能需要分多轮进行陈述，一方面，用户在对话过程中可以不断修改或完善自己的需求，另一方面，当用户的陈述的需求不够具体或明确的时候，机器也可以通过询问、澄清或确认来帮助用户找到满意的结果。如下图所示，DM 的输入就是用户输入的语义表达（或者说是用户行为，是 NLU 的输出）和当前对话状态，输出就是下一步的系统行为和更新的对话状态。这是一个循环往复不断流转直至完成任务的过程。\n",
    "\n",
    "从本质上来说，任务驱动的对话管理实际就是一个决策过程，系统在对话过程中不断根据当前状态决定下一步应该采取的最优动作（如：提供结果，询问特定限制条件，澄清或确认需求等），从而最有效的辅助用户完成信息或服务获取的任务。对话管理的任务大致有：\n",
    "* 对话状态维护（dialog state tracking, DST）\n",
    "对话状态是指记录了哪些槽位已经被填充、下一步该做什么、填充什么槽位，还是进行何种操作。用数学形式表达为，t+1 时刻的对话状态S(t+1)，依赖于之前时刻 t 的状态St，和之前时刻 t 的系统行为At，以及当前时刻 t+1 对应的用户行为O(t+1)。可以写成S(t+1)←St+At+O(t+1)。\n",
    "* 生成系统决策（dialog policy）\n",
    "根据 DST 中的对话状态（DS），产生系统行为（dialog act），决定下一步做什么 dialog act 可以表示观测到的用户输入（用户输入 -> DA，就是 NLU 的过程），以及系统的反馈行为（DA -> 系统反馈，就是 NLG 的过程）。\n",
    "* 作为接口与后端/任务模型进行交互\n",
    "\n",
    "Rasa的故事是一种训练数据的形式，用来训练Rasa的对话管理模型。故事是用户和人工智能助手之间的对话的表示，转换为特定的格式，其中用户输入表示为相应的意图(和必要的实体)，而助手的响应表示为相应的操作名称。Rasa核心对话系统的一个训练示例称为一个故事\n",
    "* Rasa故事集是用来训练Rasa对话管理模型的训练数据集格式。\n",
    "Rasa核心对话系统中的一个训练示例被叫做一个故事（story）。\n",
    "##### 格式\n",
    "这里是Rasa故事格式的一个例子：\n",
    "```json\n",
    "## greet + location/price + cuisine + num people    <!-- name of the story - just for debugging -->\n",
    "* greet\n",
    "   - action_ask_howcanhelp\n",
    "* inform{\"location\": \"rome\", \"price\": \"cheap\"}  <!-- user utterance, in format intent{entities} -->\n",
    "   - action_on_it\n",
    "   - action_ask_cuisine\n",
    "* inform{\"cuisine\": \"spanish\"}\n",
    "   - action_ask_numpeople        <!-- action that the bot should execute -->\n",
    "* inform{\"people\": \"six\"}\n",
    "   - action_ack_dosearch\n",
    "```\n",
    "* 一个故事以一个名字开始，如## greet + location/price + cuisine + num people。可以将名字命名成任何想要的文字，但是一个合理的名字有助于后期调试。\n",
    "* 一个故事的结尾是newline，然后另一个故事会以##开始。\n",
    "* 用户输入的消息以*打头，后面可以跟上实体的名称和值，如intent{\"entity1\": \"value\", \"entity2\":\"value\"}。\n",
    "* 机器人执行的响应以-打头，后面跟着动作的名称。\n",
    "* 动作返回的事件，在该动作之后，如，如果一个动作返回一个SlotSet事件，那么写成slot{\"slot_name\": \"value\"}。\n",
    "\n",
    "* 当写故事的时候，没有必要处理用户发的消息中特殊的内容。相反，可以利用NLU管道的输出，它允许使用意图和实体的组合来讲用户的可以发送的所有可能的消息映射到相同的内容。\n",
    "这里引入实体也是很重要的，因为用来预测下一个行为的策略是基于意图和实体的组合。（可以使用use_entities属性改变这个设定）。\n",
    "\n",
    "* 在写故事的时候，会遇到两种类型的动作：对话和自定义的动作。对话是机器能够处理的硬编码的消息。自定义行为，是添加了自定义的执行代码。\n",
    "所有的动作都是以-开头的。\n",
    "所有的对话动作都是以utter_开头的，并且必须和domain中定义的模板匹配。\n",
    "针对自定义动作，动作的名字可以从自定义动作类的name方法返回。尽管，对于这个命名没有规则上的限制，但是最好的实践方式是以action_开头。\n",
    "\n",
    "```json\n",
    "## story_1_search_treat_simple    <!-- name of the story - just for debugging -->\n",
    "* greet\n",
    "   - utter_greet\n",
    "* search_treat{\"disease\": \"百日咳\"}\n",
    "   - action_search_treat\n",
    "* bye\n",
    "   - utter_goodbye\n",
    "\n",
    "## story_2_search_food_simple\n",
    "* greet\n",
    "   - utter_greet\n",
    "* search_food{\"disease\": \"百日咳\"}\n",
    "   - action_search_food\n",
    "* bye\n",
    "   - utter_goodbye\n",
    "\n",
    "## story_3_search_symptom_simple\n",
    "* greet\n",
    "   - utter_greet\n",
    "* search_symptom{\"disease\": \"百日咳\"}\n",
    "   - action_search_symptom\n",
    "* bye\n",
    "   - utter_goodbye\n",
    "\n",
    "## story_4_search_cause_simple\n",
    "* greet\n",
    "   - utter_greet\n",
    "* search_cause{\"disease\": \"百日咳\"}\n",
    "   - action_search_cause\n",
    "* bye\n",
    "   - utter_goodbye\n",
    "\n",
    "## story_5_search_neopathy_simple\n",
    "* greet\n",
    "   - utter_greet\n",
    "* search_neopathy{\"disease\": \"百日咳\"}\n",
    "   - action_search_neopathy\n",
    "* bye\n",
    "   - utter_goodbye\n",
    "\n",
    "## story_6_search_drug_simple\n",
    "* greet\n",
    "   - utter_greet\n",
    "* search_drug{\"disease\": \"百日咳\"}\n",
    "   - action_search_drug\n",
    "* bye\n",
    "   - utter_goodbye\n",
    "\n",
    "## story_7_search_prevention_simple\n",
    "* greet\n",
    "   - utter_greet\n",
    "* search_prevention{\"disease\": \"百日咳\"}\n",
    "   - action_search_prevention\n",
    "* bye\n",
    "   - utter_goodbye\n",
    "\n",
    "## story_8_search_drug_func_simple\n",
    "* greet\n",
    "   - utter_greet\n",
    "* search_drug_func{\"drug\": \"头孢地尼分散片\"}\n",
    "   - action_search_drug_func\n",
    "* bye\n",
    "   - utter_goodbye\n",
    "\n",
    "## story_9_search_disease_treat_time_simple\n",
    "* greet\n",
    "   - utter_greet\n",
    "* search_disease_treat_time{\"disease\": \"百日咳\"}\n",
    "   - action_search_disease_treat_time\n",
    "* bye\n",
    "   - utter_goodbye\n",
    "\n",
    "## story_10_search_easy_get_simple\n",
    "* greet\n",
    "   - utter_greet\n",
    "* search_easy_get{\"disease\": \"百日咳\"}\n",
    "   - action_search_easy_get\n",
    "* bye\n",
    "   - utter_goodbye\n",
    "\n",
    "## story_11_search_disease_dept_simple\n",
    "* greet\n",
    "   - utter_greet\n",
    "* search_disease_dept{\"disease\": \"百日咳\"}\n",
    "   - action_search_disease_dept\n",
    "* bye\n",
    "   - utter_goodbye\n",
    "\n",
    "## story_120\n",
    "* first\n",
    "   - utter_greet\n",
    "\n",
    "## story_12\n",
    "* greet\n",
    "   - utter_greet\n",
    "   \n",
    "## story_13\n",
    "* bye\n",
    "   - utter_goodbye\n",
    "\n",
    "## story_14\n",
    "* search_treat{\"disease\": \"过敏性皮炎\"}\n",
    "   - action_search_treat\n",
    "   \n",
    "## story_15\n",
    "* search_food{\"disease\": \"过敏性皮炎\"}\n",
    "   - action_search_food\n",
    "   \n",
    "## story_16\n",
    "* search_symptom{\"disease\": \"过敏性皮炎\"}\n",
    "   - action_search_symptom\n",
    "   \n",
    "## story_17\n",
    "* search_cause{\"disease\": \"过敏性皮炎\"}\n",
    "   - action_search_cause\n",
    "   \n",
    "## story_18\n",
    "* search_neopathy{\"disease\": \"过敏性皮炎\"}\n",
    "   - action_search_neopathy\n",
    "   \n",
    "## story_19\n",
    "* search_drug{\"disease\": \"过敏性皮炎\"}\n",
    "   - action_search_drug\n",
    "   \n",
    "## story_20\n",
    "* search_prevention{\"disease\": \"过敏性皮炎\"}\n",
    "   - action_search_prevention\n",
    "   \n",
    "## story_21\n",
    "* search_drug_func{\"drug\": \"复方斑蝥胶囊\"}\n",
    "   - action_search_drug_func\n",
    "   \n",
    "## story_22\n",
    "* search_disease_treat_time{\"disease\": \"过敏性皮炎\"}\n",
    "   - action_search_disease_treat_time\n",
    "   \n",
    "## story_23\n",
    "* search_easy_get{\"disease\": \"过敏性皮炎\"}\n",
    "   - action_search_easy_get\n",
    "   \n",
    "## story_24\n",
    "* search_disease_dept{\"disease\": \"过敏性皮炎\"}\n",
    "   - action_search_disease_dept\n",
    "   \n",
    "```\n",
    "因此我们归纳出：\n",
    "Story格式大致包含三个部分：\n",
    "\n",
    "* 用户输入（User Messages）\n",
    " 使用*开头的语句表示用户的输入消息，我们无需使用包含某个具体内容的输入，而是使用NLU管道输出的intent和entities来表示可能的输入。需要注意的是，如果用户的输入可能包含entities，建议将其包括在内，将有助于policies预测下一步action。这部分大致包含三种形式，示例如下：\n",
    "\n",
    "（1）* greet 表示用户输入没有entity情况；\n",
    "（2）* inform{\"people\": \"six\"} 表示用户输入包含entity情况，响应这类intent为普通action；\n",
    "（3）* request_weather 表示用户输入Message对应的intent为form action情况；\n",
    "\n",
    "* 动作（Actions）\n",
    " 使用-开头的语句表示要执行动作(Action)，可分为utterance actions和custom actions，其中，前者在domain.yaml中定义以utter_为前缀，比如名为greet的意图，它的回复应为utter_greet；后者为自定义动作，具体逻辑由我们自己实现，虽然在定义action名称的时候没有限制，但是还是建议以action_为前缀，比如名为inform的意图fetch_profile的意图，它的response可为action_fetch_profile。\n",
    "\n",
    "* 事件（Events）\n",
    " Events也是使用-开头，主要包含槽值设置(SlotSet)和激活/注销表单(Form)，它是是Story的一部分，并且必须显示的写出来。Slot Events和Form Events的作用如下："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Story设计\n",
    "从上面的story可以看到，书写story需要将所有的意图都覆盖，并且每个意图应该执行什么action需要人为去干预，如果意图数量多了，这将是一个繁琐的工作。\n",
    "因此rasa提供了一套工具 rasa x(非开源),可以通过每次用户输入，然后让用户选择需要执行的action，当一个流程完成后，自动就把刚才执行的流程写成一个story，整个过程可以图形化界面操作，非常方便。另外也可以通过rasa的在线交互学习完成上述步骤。\n",
    "```json\n",
    "备注：rasa x安装因为网络原因，不能保证每个人的机器都能一次性装好，可以使用交互学习命令行模式了解其功能\n",
    "```\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/05fb36ac774745f0b92c2e8292c581228aae97c059994f31abb9475b58dcfbdd)\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/294ee5f642544969a3c8ee2862539a1a3115ffefe6bd40a7a8fd2a4ec32907c2)\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/dfde7fd790454eadac2955715be1af9a2741d4d545114aab9f1d9344d1833bda)\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/5f11ed452d724223842295465c967eac6215d194a9e549fea5a59b185d367f1d)\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/6c00dd4b9f38412ea5bdadce087cb644f61c359cea7f437cb4c122288f03b413)\n",
    "\n",
    "* 启动action之后，执行：\n",
    "```\n",
    "rasa interactive\n",
    "```\n",
    "即可开启命令行模式（需要安装依赖 pip install mitie）\n",
    "\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/924b2533a85249fc91d041638868cf5a66e4b2fdc1244d7381f130c267fa0296)\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/a94b6a0c69c14cb5bee367e8636a28dbcb5d292fc97e4ead80f81dbab7e57e8f)\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/1cbcd0ab807244088cd30dd78c6e1b5b3f26b83904d14c3b8f0fea56cff226ff)\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/b6b3d225fb1b4d01bb85bc8a11451fb97b577508c60a41a680f3eb29430d60d2)\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/5c2d9057637d42b782a75924e70ca00d04b073f2df4e4040ab205b0410015ac5)\n",
    "运行完了之后，会在项目目录下多一份story文件，在设计自己的story的时候，可以参考这份文件。\n",
    "```json\n",
    "特别对于一些form表单的action，比如一个表单需要搜集客户信息，用户表达这个意图之后，需要机器人不断搜集客户信息，例如姓名，职业，年龄，教育背景，这种时候设计的slot比较繁杂，story相对要复杂一些，rasa官方也推荐用interactive learning来书写story\n",
    "参考地址：https://legacy-docs-v1.rasa.com/core/interactive-learning/\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 词向量与config文件详解\n",
    "\n",
    "* MitieNLP，也是我们本教程使用的组件\n",
    "\n",
    "| MitieNLP  | 说明  |\n",
    "| -------- | -------- |\n",
    "| Short     | MITIE initializer，即Mitie是MITIE initializer的简称。     |\n",
    "| Outputs     | 无     |\n",
    "| Requires     | 无     |\n",
    "| Description     | 初始化mitie结构。每个mitie组件都依赖于此，因此应该将其放在任何使用mitie组件的每个管道(pipline)的开头     |\n",
    "| Configuration     | MITIE 需要一个语言模型(.dat)，且必须在configs.yml配置中指定。示例如下：     |\n",
    "\n",
    "在configs.yml中应如下配置：\n",
    "```\n",
    "pipeline:\n",
    "- name: \"MitieNLP\"\n",
    "  # 语言模型\n",
    "  model: \"data/total_word_feature_extractor_zh.dat\"\n",
    "```\n",
    "\n",
    "其他的词向量组件我们也可以了解下：\n",
    "* SpacyNLP\n",
    "\n",
    "| SpacyNLP  | 说明  |\n",
    "| -------- | -------- |\n",
    "| Short     | spacy language initializer，即spacy语言初始化。     |\n",
    "| Outputs     | 无     |\n",
    "| Requires     | 无     |\n",
    "| Description     | 初始化spacy结构。每个spacy组件都依赖于此，因此应该将其放在任何使用mitie组件的每个管道(pipline)的开头     |\n",
    "| Configuration     |Spacy需要配置语言模型，默认将使用配置的语言。如果要使用的spacy模型的名称不同于language标记(“en”、“de”等)，则可以使用配置变量指定模型名称，将名称将传递给模型：space.load(name)。示例如下：     |\n",
    "\n",
    "在configs.yml中应如下配置：\n",
    "```\n",
    "pipeline:\n",
    "- name: \"SpacyNLP\"\n",
    "  # 指定语言模型\n",
    "  model: \"en_core_web_md\"\n",
    "  # 设定在检索单词向量时，这将决定单词的大小写是否相关\n",
    "  # 当为false时，表示不区分大小写。比如`hello` and `Hello`\n",
    "  # 检索到的向量是相同的。\n",
    "  case_sensitive: false\n",
    "```\n",
    "\n",
    "更多参考，请参考官网：https://legacy-docs-v1.rasa.com/nlu/components/#mitienlp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenizers\n",
    "更多参考，请参考官网：https://legacy-docs-v1.rasa.com/nlu/components/#tokenizers\n",
    "\n",
    "* WhitespaceTokenizer\n",
    "\n",
    "| WhitespaceTokenizer  | 说明  |\n",
    "| -------- | -------- |\n",
    "| Short     | 分词器以空格(whitespaces)作为分词间隔    |\n",
    "| Outputs     | 无     |\n",
    "| Requires     | 无     |\n",
    "| Description     | 为每个以空格分隔的字符序列创建token，得到token可用于MITIE实体提取器。    |\n",
    "| Configuration     | 如果想把意图分成多个标签，例如，为了预测多个意图或为分层的意图结构建模，使用intent_split_symbol标志。可以通过case_sensitive设置是否大小写敏感，默认true(敏感)。示例如下：    |\n",
    "\n",
    "在configs.yml中应如下配置：\n",
    "```\n",
    "pipeline:\n",
    "- name: \"WhitespaceTokenizer\"\n",
    "  # 指定是否大小写敏感，默认true为敏感\n",
    "  case_sensitive: false\n",
    "```\n",
    "\n",
    "* JiebaTokenizer\n",
    "\n",
    "| JiebaTokenizer  | 说明  |\n",
    "| -------- | -------- |\n",
    "| Short     | 使用Jieba作为 Tokenizer，对中文进行分词   |\n",
    "| Outputs     | 无     |\n",
    "| Requires     | 无     |\n",
    "| Description     | 用于中文的Tokenizer，对于其他语种Jieba会如WhitespaceTokenizer般工作。JiebaTokenizer可为MITIE实体抽取器定义token。    |\n",
    "| Configuration     | 用户的自定义字典文件可以通过特定的文件目录路径dictionary_path自动加载，但是需要在配置文件中进行配置。示例如下：   |\n",
    "\n",
    "在configs.yml中应如下配置：\n",
    "```\n",
    "pipeline:\n",
    "- name: \"JiebaTokenizer\"\n",
    "  # 指定自定义词典\n",
    "  dictionary_path: \"path/to/custom/dictionary/dir\"\n",
    "```\n",
    "\n",
    "* MitieTokenizer\n",
    "\n",
    "| MitieTokenizer  | 说明  |\n",
    "| -------- | -------- |\n",
    "| Short     | 使用Mitie进行分词   |\n",
    "| Outputs     | 无     |\n",
    "| Requires     | 无     |\n",
    "| Description     |  用MITIE tokenizer创建tokens，从而服务于 MITIE 实体抽取 |\n",
    "| Configuration     |    |\n",
    "\n",
    "在configs.yml中应如下配置：\n",
    "```\n",
    "pipeline:\n",
    "- name: \"MitieTokenizer\"\n",
    "```\n",
    "\n",
    "* SpacyTokenizer\n",
    "\n",
    "| SpacyTokenizer  | 说明  |\n",
    "| -------- | -------- |\n",
    "| Short     | 使用Spacy进行分词   |\n",
    "| Outputs     | 无     |\n",
    "| Requires     | 无     |\n",
    "| Description     |  用Spacytokenizer创建tokens，从而服务于Spacy 实体抽取 |\n",
    "| Configuration     |    |\n",
    "\n",
    "在configs.yml中应如下配置：\n",
    "```\n",
    "pipeline:\n",
    "- name: \"SpacyTokenizer\"\n",
    "```\n",
    "\n",
    "* ConveRTTokenizer\n",
    "\n",
    "| ConveRTTokenizer  | 说明  |\n",
    "| -------- | -------- |\n",
    "| Short     | 使用ConveRt进行分词   |\n",
    "| Outputs     | 无     |\n",
    "| Requires     | 无     |\n",
    "| Description     |  用ConveRT Tokenizer创建tokens，从而服务于ConveRTFeaturizer 实体抽取 |\n",
    "| Configuration     |    |\n",
    "\n",
    "在configs.yml中应如下配置：\n",
    "```\n",
    "pipeline:\n",
    "- name: \"ConveRTTokenizer\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 文本特征化（Text Featurizers）\n",
    "\n",
    "* MitieFeaturizer\n",
    "\n",
    "| ConveRTTokenizer  | 说明  |\n",
    "| -------- | -------- |\n",
    "| Short     | MITIE intent featurizer，即使用MITIE特征化意图信息   |\n",
    "| Outputs     | 无，作为意图分类器的输入(例如SklearnIntentClassifier)     |\n",
    "| Requires     | 需要先配置MitieNLP     |\n",
    "|    Type  |  稠密featurizer |\n",
    "| Description     |  使用MITIE featurizer为意图分类创建特征。需要注意的是：MitieIntentClassifier组件中并没有使用。目前，只有SklearnIntentClassifier能够使用预先计算的特性。  |\n",
    "\n",
    "在configs.yml中应如下配置：\n",
    "```\n",
    "pipeline:\n",
    "- name: \"MitieFeaturizer\"\n",
    "```\n",
    "\n",
    "* SpacyFeaturizer\n",
    "\n",
    "| SpacyFeaturizer  | 说明  |\n",
    "| -------- | -------- |\n",
    "| Short     | spacy intent featurizer，即使用Spacy特征化意图信息   |\n",
    "| Outputs     | 无，作为意图分类器的输入(例如SklearnIntentClassifier)     |\n",
    "| Requires     | 需要先配置SpacyNLP     |\n",
    "|    Type  |  稠密featurizer |\n",
    "| Description     |  使用spacy featurizer为意图分类创建特征  |\n",
    "\n",
    "在configs.yml中应如下配置：\n",
    "```\n",
    "pipeline:\n",
    "- name: \"SpacyFeaturizer\"\n",
    "```\n",
    "\n",
    "* ConveRTFeaturizer\n",
    "\n",
    "| ConveRTFeaturizer  | 说明  |\n",
    "| -------- | -------- |\n",
    "| Short     | 使用ConveRT模型创建用户消息和响应(如果指定的话)的向量表示   |\n",
    "| Outputs     | 无，作为意图分类器和response selectors的输入，分别对应意图特征和响应特征。比如EmbeddingIntentClassifier和ResponseSelector   |\n",
    "| Requires     | 需要配置ConveRTTokenizer  |\n",
    "|    Type  |  稠密featurizer |\n",
    "| Description     |  为意图分类和响应选择创建特征，使用默认签名来计算输入文本的向量表示。 需要注意：(1)由于ConveRT模型仅在英语语料上训练，因此只有当训练数据是英语语言时才能使用这个featurizer。 (2)使用之前需要安装tensorflow_text和tensorflow_hub)，可以通过pip install rasa[convert]来安装。 |\n",
    "\n",
    "在configs.yml中应如下配置：\n",
    "```\n",
    "pipeline:\n",
    "- name: \"ConveRTFeaturizer\"\n",
    "```\n",
    "\n",
    "* RegexFeaturizer\n",
    "\n",
    "| RegexFeaturizer  | 说明  |\n",
    "| -------- | -------- |\n",
    "| Short     | 创建正则表达式特征以支持意图和实体分类   |\n",
    "| Outputs     | text_features 和 tokens.pattern  |\n",
    "| Requires     | 需要配置ConveRTTokenizer  |\n",
    "|    Type  |  稀疏 featurizer |\n",
    "| Description     |  为实体提取和意图分类创建特征。在训练期间，regex intent featurizer 以训练数据的格式创建一系列正则表达式列表。对于每个正则，都将设置一个特征，标记是否在输入中找到该表达式，然后将其输入到intent classifier / entity extractor 中以简化分类(假设分类器在训练阶段已经学习了该特征集合，该特征集合表示一定的意图)。将Regex特征用于实体提取目前仅CRFEntityExtractor组件支持。注意：在 regex featurizer 之前，需要先进行tokenizer操作。 |\n",
    "\n",
    "在configs.yml中应如下配置：\n",
    "```\n",
    "pipeline:\n",
    "- name: \"RegexFeaturizer\"\n",
    "```\n",
    "\n",
    "* CountVectorsFeaturizer\n",
    "\n",
    "| CountVectorsFeaturizer  | 说明  |\n",
    "| -------- | -------- |\n",
    "| Short     | 创建用户信息和标签(意图和响应)的词袋表征   |\n",
    "| Outputs     | 无，用作意图分类器的输入，输入的意图特征以词袋表征(如EmbeddingIntentClassifier)  |\n",
    "| Requires     | 无  |\n",
    "|    Type  |  稀疏 featurizer |\n",
    "| Description     |  为意图分类和 response selection创建特征。使用sklearn的CountVectorizer创建用户消息和标签特征的词袋表征。所有token仅由数字组成(如123和99，但不会存在a123d)将被分配到相同的功能。 |\n",
    "\n",
    "在configs.yml中应如下配置：\n",
    "```\n",
    "pipeline:\n",
    "- name: \"CountVectorsFeaturizer\"\n",
    "  \"use_shared_vocab\": False,\n",
    "  analyzer: 'word' \n",
    "  token_pattern: r'(?u)\\b\\w\\w+\\b'\n",
    "  strip_accents: None  \n",
    "  stop_words: None \n",
    "  min_df: 1 \n",
    "  max_df: 1.0 \n",
    "  min_ngram: 1  \n",
    "  max_ngram: 1  \n",
    "  max_features: None  \n",
    "  lowercase: true\n",
    "  OOV_token: None  \n",
    "  OOV_words: []  \n",
    "```\n",
    "更多参考，请访问官网：\n",
    "https://legacy-docs-v1.rasa.com/nlu/components/#text-featurizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 意图分类\n",
    "\n",
    "* MitieIntentClassifier\n",
    "\n",
    "| MitieIntentClassifier  | 说明  |\n",
    "| -------- | -------- |\n",
    "| Short     | MITIE intent classifier(使用text categorizer)  |\n",
    "| Outputs     | intent  |\n",
    "| Requires     | tokenizer 和 featurizer  |\n",
    "|    Type  |  稀疏 featurizer |\n",
    "| Description     |  该分类器使用MITIE进行意图分类。底层分类器使用的是具有稀疏线性核的多类线性支持向量机(可以查看MITIE trainer code)。 |\n",
    "\n",
    "在configs.yml中应如下配置：\n",
    "```\n",
    "pipeline:\n",
    "- name: \"MitieIntentClassifier\"\n",
    "```\n",
    "输出示例：\n",
    "```\n",
    "{\n",
    "    \"intent\": {\"name\": \"greet\", \"confidence\": 0.98343}\n",
    "}\n",
    "```\n",
    "\n",
    "* SklearnIntentClassifier\n",
    "\n",
    "| SklearnIntentClassifier  | 说明  |\n",
    "| -------- | -------- |\n",
    "| Short     | sklearn intent classifier |\n",
    "| Outputs     | intent 和 intent_ranking  |\n",
    "| Requires     | A featurizer  |\n",
    "| Description     |  该sklearn意图分类器训练一个线性支持向量机，该支持向量机通过网格搜索得到优化。除了其他分类器，它还提供没有“获胜”的标签的排名。spacy意图分类器需要在管道中的先加入一个featurizer。该featurizer创建用于分类的特征。 |\n",
    "\n",
    "在configs.yml中应如下配置：\n",
    "```\n",
    "pipeline:\n",
    "- name: \"SklearnIntentClassifier\"\n",
    "  # 指定SVM训练时要尝试的参数\n",
    "  # 通过运行超参数搜索，以找到最佳的参数集\n",
    "  C: [1, 2, 5, 10, 20, 100]\n",
    "  # 指定C-SVM使用的内核\n",
    "  # 它与GridSearchCV中的“C”超参数一起使用\n",
    "  kernels: [\"linear\"]\n",
    "```\n",
    "输出示例：\n",
    "```\n",
    "{\n",
    "    \"intent\": {\"name\": \"greet\", \"confidence\": 0.78343},\n",
    "    \"intent_ranking\": [\n",
    "        {\n",
    "            \"confidence\": 0.1485910906220309,\n",
    "            \"name\": \"goodbye\"\n",
    "        },\n",
    "        {\n",
    "            \"confidence\": 0.08161531595656784,\n",
    "            \"name\": \"restaurant_search\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "```\n",
    "\n",
    "* EmbeddingIntentClassifier\n",
    "\n",
    "| EmbeddingIntentClassifier  | 说明  |\n",
    "| -------- | -------- |\n",
    "| Short     | Embedding intent classifier |\n",
    "| Outputs     | intent 和 intent_ranking  |\n",
    "| Requires     | A featurizer  |\n",
    "| Description     |  嵌入式意图分类器将用户输入和意图标签嵌入到同一空间中。Supervised embeddings通过最大化它们之间的相似性来训练。该算法基于StarSpace的。但是，在这个实现中，损失函数略有不同，添加了额外的隐藏层和dropout。该算法还提供了未“获胜”标签的相似度排序。在embedding intent classifier之前，需要在管道中加入一个featurizer。该featurizer创建用以embeddings的特征。建议使用CountVectorsFeaturizer，它可选的预处理有SpacyNLP和SpacyTokenizer。 |\n",
    "\n",
    "在configs.yml中应如下配置：\n",
    "```\n",
    "pipeline:\n",
    "- name: \"EmbeddingIntentClassifier\"\n",
    "# Embedding算法的控制参数非常多\n",
    "# 具体参照官方文档，这里以指定训练次数为例\n",
    "  epochs: 500\t\n",
    "```\n",
    "输出示例：\n",
    "```\n",
    "{\n",
    "    \"intent\": {\"name\": \"greet\", \"confidence\": 0.8343},\n",
    "    \"intent_ranking\": [\n",
    "        {\n",
    "            \"confidence\": 0.385910906220309,\n",
    "            \"name\": \"goodbye\"\n",
    "        },\n",
    "        {\n",
    "            \"confidence\": 0.28161531595656784,\n",
    "            \"name\": \"restaurant_search\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "```\n",
    "\n",
    "* KeywordIntentClassifier\n",
    "\n",
    "| KeywordIntentClassifier  | 说明  |\n",
    "| -------- | -------- |\n",
    "| Short     | 简单的关键字匹配意图分类器，适于小型、短期的项目 |\n",
    "| Outputs     | intent   |\n",
    "| Requires     | nothing |\n",
    "| Description     |  该分类器通过搜索关键字的消息来工作。默认情况下，匹配是大小写敏感的，只精确匹配地搜索用户消息中关键字。意图的关键字是NLU训练数据中意图的例子。这意味着整个示例是关键字，而不是示例中的单个单词。注意：此分类器仅用于小型项目或入门级项目。如果你有很少的NLU训练数据，则可以试试管道选择中一个管道。 |\n",
    "\n",
    "在configs.yml中应如下配置：\n",
    "```\n",
    "pipeline:\n",
    "- name: \"KeywordIntentClassifier\"\n",
    "  case_sensitive: True\n",
    "```\n",
    "输出示例：\n",
    "```\n",
    "{\n",
    "    \"intent\": {\"name\": \"greet\", \"confidence\": 1.0}\n",
    "}\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 选择器（Selectors）：主要用于FAQ\n",
    "* Response Selector\n",
    "\n",
    "| Response Selector  | 说明  |\n",
    "| -------- | -------- |\n",
    "| Short     | 一个字典，关键字为direct_response_intent，value属性包含response和ranking |\n",
    "| Outputs     | intent   |\n",
    "| Requires     | A featurizer |\n",
    "| Description     |  主要用于FAQ |\n",
    "\n",
    "在configs.yml中应如下配置：\n",
    "```\n",
    "\n",
    "```\n",
    "输出示例：\n",
    "```\n",
    "{\n",
    "    \"response_selector\": {\n",
    "      \"faq\": {\n",
    "        \"response\": {\"confidence\": 0.7356462617, \"name\": \"Supports 3.5, 3.6 and 3.7, recommended version is 3.6\"},\n",
    "        \"ranking\": [\n",
    "            {\"confidence\": 0.7356462617, \"name\": \"Supports 3.5, 3.6 and 3.7, recommended version is 3.6\"},\n",
    "            {\"confidence\": 0.2134543431, \"name\": \"You can ask me about how to get started\"}\n",
    "        ]\n",
    "      }\n",
    "    }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 实体提取\n",
    "\n",
    "* MitieEntityExtractor\n",
    "\n",
    "| MitieEntityExtractor  | 说明  |\n",
    "| -------- | -------- |\n",
    "| Short     | MITIE entity extraction (using a MITIE NER trainer) |\n",
    "| Outputs     | entities   |\n",
    "| Requires     | 需要先配置MitieNLP |\n",
    "| Description     |  用 MITIE entity extraction抽取语句中的实体。底层分类器使用具有稀疏线性核和自定义特征的多类线性支持向量机。该MITIE组件不提供实体置信值。 |\n",
    "\n",
    "在configs.yml中应如下配置：\n",
    "```\n",
    "pipeline:\n",
    "- name: \"MitieEntityExtractor\"\n",
    "```\n",
    "输出示例：\n",
    "```\n",
    "{\n",
    "    \"entities\": [{\"value\": \"New York City\",\n",
    "                  \"start\": 20,\n",
    "                  \"end\": 33,\n",
    "                  \"confidence\": null,\n",
    "                  \"entity\": \"city\",\n",
    "                  \"extractor\": \"MitieEntityExtractor\"}]\n",
    "}\n",
    "```\n",
    "\n",
    "* SpacyEntityExtractor\n",
    "\n",
    "| SpacyEntityExtractor  | 说明  |\n",
    "| -------- | -------- |\n",
    "| Short     | spaCy entity extraction |\n",
    "| Outputs     | entities   |\n",
    "| Requires     | 需要先配置SpacyNLP |\n",
    "| Description     |  该组件使用spaCy来预测消息的实体。spacy使用统计BILOU转移模型。到目前为止，该组件只能使用spacy内置的实体提取模型，不能进行再训练。此提取器不提供任何置信评分。配置spacy组件应该提取哪些维度，比如实体类型。可用维度的完整列表可以在spaCy文档中找到。不指定维度选项将提取所有可用维度。 |\n",
    "\n",
    "在configs.yml中应如下配置：\n",
    "```\n",
    "pipeline:\n",
    "- name: \"SpacyEntityExtractor\"\n",
    "  # dimensions to extract\n",
    "  dimensions: [\"PERSON\", \"LOC\", \"ORG\", \"PRODUCT\"]\n",
    "```\n",
    "输出示例：\n",
    "```\n",
    "{\n",
    "    \"entities\": [{\"value\": \"New York City\",\n",
    "                  \"start\": 20,\n",
    "                  \"end\": 33,\n",
    "                  \"entity\": \"city\",\n",
    "                  \"confidence\": null,\n",
    "                  \"extractor\": \"SpacyEntityExtractor\"}]\n",
    "}\n",
    "```\n",
    "\n",
    "* EntitySynonymMapper\n",
    "\n",
    "| EntitySynonymMapper  | 说明  |\n",
    "| -------- | -------- |\n",
    "| Short     | 将同义词映射到同一个值 |\n",
    "| Outputs     | 修改以前的实体提取组件找到的现有实体   |\n",
    "| Requires     | 无 |\n",
    "| Description     |  如果训练数据包含已定义的同义词(通过对实体示例使用value属性)。此组件将确保检测到的实体值映射到相同的值。 |\n",
    "\n",
    "在configs.yml中应如下配置：\n",
    "```\n",
    "pipeline:\n",
    "- name: \"EntitySynonymMapper\"\n",
    "```\n",
    "输出示例：\n",
    "```\n",
    "[{\n",
    "  \"text\": \"I moved to New York City\",\n",
    "  \"intent\": \"inform_relocation\",\n",
    "  \"entities\": [{\"value\": \"nyc\",\n",
    "                \"start\": 11,\n",
    "                \"end\": 24,\n",
    "                \"entity\": \"city\",\n",
    "               }]\n",
    "},\n",
    "{\n",
    "  \"text\": \"I got a new flat in NYC.\",\n",
    "  \"intent\": \"inform_relocation\",\n",
    "  \"entities\": [{\"value\": \"nyc\",\n",
    "                \"start\": 20,\n",
    "                \"end\": 23,\n",
    "                \"entity\": \"city\",\n",
    "               }]\n",
    "}]\n",
    "```\n",
    "\n",
    "* CRFEntityExtractor\n",
    "\n",
    "| CRFEntityExtractor  | 说明  |\n",
    "| -------- | -------- |\n",
    "| Short     | 条件随机场实体抽取器 |\n",
    "| Outputs     | entities   |\n",
    "| Requires     | A tokenizer |\n",
    "| Description     |  此组件使用条件随机场来进行命名实体识别。CRFs可以被认为是一个无向的马尔可夫链，其中时间步长是单词，状态是实体类别。单词的特征(大写，词性标注POS，等等)给出了特定实体类别的概率，就像相邻实体标记之间的转换一样：然后计算并返回最可能的标记结果。如果使用POS功能(pos或pos2)，则必须安装spaCy。如果想使用额外的功能，如预训练的词嵌入，稠密的featurizer，则可以使用“text_dense_features”。确保在相应的featurizer中将“return_sequence”设置为True。 |\n",
    "\n",
    "在configs.yml中应如下配置：\n",
    "```\n",
    "pipeline:\n",
    "- name: \"CRFEntityExtractor\"\n",
    "  features: [[\"low\", \"title\"], [\"bias\", \"suffix3\"], [\"upper\", \"pos\", \"pos2\"]]\n",
    "  # 决定是否使用BILOU_flag\n",
    "  BILOU_flag: true\n",
    "  # 在训练前将该参数设定给sklearn_crfcuite.CRF tagger\n",
    "  max_iterations: 50\n",
    "  # 指定L1正则化系数\n",
    "  # 在训练前将该参数设定给sklearn_crfcuite.CRF tagger\n",
    "  L1_c: 0.1\n",
    "  # 指定L2正则化系数\n",
    "  # 在训练前将该参数设定给sklearn_crfcuite.CRF tagger\n",
    "  L2_c: 0.1\n",
    "```\n",
    "输出示例：\n",
    "```\n",
    "{\n",
    "    \"entities\": [{\"value\":\"New York City\",\n",
    "                  \"start\": 20,\n",
    "                  \"end\": 33,\n",
    "                  \"entity\": \"city\",\n",
    "                  \"confidence\": 0.874,\n",
    "                  \"extractor\": \"CRFEntityExtractor\"}]\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DIETClassifier（意图与实体提取联合抽取器）\n",
    "* DIETClassifier\n",
    "\n",
    "| DIETClassifier  | 说明  |\n",
    "| -------- | -------- |\n",
    "| Short     | Dual Intent Entity Transformer (DIET)来做意图提取和实体提取 |\n",
    "| Outputs     | entities, intent and intent_ranking   |\n",
    "| Requires     | dense_features and/or sparse_features |\n",
    "| Description     |  DIET（双重意图和实体转换器）是用于意图分类和实体识别的多任务体系结构。该架构基于两个任务共享的Transformer。实体标签序列是通过对应于令牌输入序列的转换器输出序列顶部的条件随机场（CRF）标签层进行预测的。对于意图标签，__ CLS__令牌和意图标签的转换器输出被嵌入到单个语义向量空间中。我们使用点积损失来最大化与目标标签的相似性，并最小化与阴性样品的相似性。 |\n",
    "\n",
    "在configs.yml中应如下配置：\n",
    "```\n",
    "- name: \"DIETClassifier\"\n",
    "  epochs: 100\n",
    "```\n",
    "输出示例：\n",
    "```\n",
    "{\n",
    "    \"intent\": {\"name\": \"greet\", \"confidence\": 0.8343},\n",
    "    \"intent_ranking\": [\n",
    "        {\n",
    "            \"confidence\": 0.385910906220309,\n",
    "            \"name\": \"goodbye\"\n",
    "        },\n",
    "        {\n",
    "            \"confidence\": 0.28161531595656784,\n",
    "            \"name\": \"restaurant_search\"\n",
    "        }\n",
    "    ],\n",
    "    \"entities\": [{\n",
    "        \"end\": 53,\n",
    "        \"entity\": \"time\",\n",
    "        \"start\": 48,\n",
    "        \"value\": \"2017-04-10T00:00:00.000+02:00\",\n",
    "        \"confidence\": 1.0,\n",
    "        \"extractor\": \"DIETClassifier\"\n",
    "    }]\n",
    "}\n",
    "```\n",
    "详细信息，请参考：https://legacy-docs-v1.rasa.com/nlu/components/#diet-classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rasa NLU Pipline\n",
    "在Rasa NLU模块中，提供了一种名为Pipline(管道)配置方式，传入的消息(Message)通过管道中一系列组件处理后得到最终的模型。管道(Pipline)由多个组件(Component)构成，每个组件有各自的功能，比如实体提取、意图分类、响应选择、预处理等，这些组件在管道中一个接着一个的执行，每个组件处理输入并创建输出，并且输出可以被该组件之后管道中任何组件使用。当然，有些组件只生成管道中其他组件使用的信息，有些组件生成Output属性，这些Output属性将在处理完成后返回。\n",
    "\n",
    "在Rasa NLU模块中，已为我们提供了几种模板(Template) Pipline，比如pretrained_embeddings_spacy、supervised_embeddings等，每一种Pipline组件构成不同，可以根据训练数据的特性选择使用。当然，Pipline的配置非常的灵活，我们可以自定义Pipline中的组件，实现不同特性的Pipline。\n",
    "\n",
    "* pretrained_embeddings_spacy\n",
    "\n",
    "在config.yaml文件中配置如下：\n",
    "```\n",
    "language: \"en\"\n",
    "\n",
    "pipeline: \"pretrained_embeddings_spacy\"\n",
    "```\n",
    "当然，上述配置等价于：\n",
    "```\n",
    "language: \"en\"\n",
    "\n",
    "pipeline:\n",
    "- name: \"SpacyNLP\"        # 预训练词向量        \n",
    "- name: \"SpacyTokenizer\"  # 文本分词器          \n",
    "- name: \"SpacyFeaturizer\" # 文本特征化  \n",
    "- name: \"RegexFeaturizer\" # 支持正则表达式  \n",
    "- name: \"CRFEntityExtractor\" # 实体提取器  \n",
    "- name: \"EntitySynonymMapper\" # 实体同义词映射  \n",
    "- name: \"SklearnIntentClassifier\" # 意图分类器 \n",
    "```\n",
    "\n",
    "pretrained_embeddings_spacy管道使用GloVe或 fastText的预训练词向量，因此，它的优势在于当你有一个训练样本如I want to buy apples，Rasa会预测意图为get pears。因为模型已经知道“苹果”和“梨”是非常相似的。如果没有足够大的训练数据，这一点尤其有用。\n",
    "\n",
    "* supervised_embeddings\n",
    "在config.yaml文件中配置如下：\n",
    "```\n",
    "language: \"en\"\n",
    "\n",
    "pipeline: \"supervised_embeddings\"\n",
    "```\n",
    "当然，上述配置等价于：\n",
    "```\n",
    "language: \"en\"\n",
    "\n",
    "pipeline:\n",
    "- name: \"WhitespaceTokenizer\"   # 分词器\n",
    "- name: \"RegexFeaturizer\"       # 正则\n",
    "- name: \"CRFEntityExtractor\"\t # 实体提取器\n",
    "- name: \"EntitySynonymMapper\"\t # 同义词映射\n",
    "- name: \"CountVectorsFeaturizer\"  # featurizes文本基于词\n",
    "- name: \"CountVectorsFeaturizer\"  # featurizes文本基于n-grams character，保留词边界 \n",
    "  analyzer: \"char_wb\"\n",
    "  min_ngram: 1\n",
    "  max_ngram: 4\n",
    "- name: \"EmbeddingIntentClassifier\"  # 意图分类器\n",
    "```\n",
    "supervised_embeddings 管道不使用任何的预训练词向量或句向量，而是针对自己的数据集特别做的训练。它的优势是面向自己特定数据集的词向量(your word vectors will be customised for your domain)，比如，在通用英语中，单词“balance” (平衡)与单词 “symmetry”(对称)意思非常相近，而与单词\"cash\"意思截然不同。但是，在银行领域(domain)，“balance”与\"cash\"意思相近，而supervised_embeddings训练得到的模型就能够捕捉到这一点。该pipline不需要任何指定的语言模型，因此适用于任何语言，当然，需要指定对应的分词器。比如默认使用WhitespaceTokenizer，对于中文可以使用Jieba分词器等等，也就是该Pipline的组件是可以自定义的。\n",
    "\n",
    "* pretrained_embeddings_convert\n",
    "在config.yaml文件中配置如下：\n",
    "```\n",
    "language: \"en\"\n",
    "\n",
    "pipeline: \"pretrained_embeddings_convert\"\n",
    "```\n",
    "当然，上述配置等价于：\n",
    "```\n",
    "language: \"en\"\n",
    "\n",
    "pipeline:\n",
    "- name: \"ConveRTTokenizer\"\n",
    "- name: \"ConveRTFeaturizer\"\n",
    "- name: \"EmbeddingIntentClassifier\"\n",
    "```\n",
    "pretrained_embeddings_convert使用预训练的句子编码模型ConveRT以抽取用户输入句子的整体向量表征。该pipeline使用ConveRT模型抽取句子表征，并将句子表征输入到EmbeddingIntentClassifier以进行意图分类。使用pretrained_embeddings_convert的好处是不独立地处理用户输入句子中的每个词，而是为完整的句子创建上下文向量表征。比如，句子can I book a car?Rasa 会预测意图为I need a ride from my place。由于这两个示例的上下文向量表征已经非常相似，因此对它们进行分类的意图很可能是相同的。如果没有足够大的训练数据，这也很有用。需要注意的是，由于ConveRT模型仅在英语语料上进行训练，因此只有在训练数据是英语时才能够使用该pipeline。\n",
    "\n",
    "更多关于pipeline的选择，请参考：https://legacy-docs-v1.rasa.com/nlu/choosing-a-pipeline/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 如何自定义开发NLU组件？\n",
    "```\n",
    "https://legacy-docs-v1.rasa.com/api/custom-nlu-components/\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**上面介绍了那么多，下面我们回到我们的项目，看看我们的项目需要怎么配置config文件呢：**\n",
    "\n",
    "```\n",
    "language: \"zh\"\n",
    "\n",
    "policies:\n",
    "  - name: \"KerasPolicy\"\n",
    "    epochs: 120\n",
    "    featurizer:\n",
    "      - name: MaxHistoryTrackerFeaturizer\n",
    "        max_history: 5\n",
    "        state_featurizer:\n",
    "          - name: BinarySingleStateFeaturizer\n",
    "  - name: \"rasa.core.policies.memoization.MemoizationPolicy\"\n",
    "    max_history: 5\n",
    "  - name: \"rasa.core.policies.mapping_policy.MappingPolicy\"\n",
    "  - name: \"rasa.core.policies.fallback.FallbackPolicy\"\n",
    "    nlu_threshold: 0.4\n",
    "    core_threshold: 0.3\n",
    "    fallback_action_name: 'action_donknow'\n",
    "\n",
    "pipeline:\n",
    "- name: \"MitieNLP\"\n",
    "  model: \"data/total_word_feature_extractor_zh.dat\"\n",
    "- name: \"JiebaTokenizer\"\n",
    "  dictionary_path: \"data/jieba_userdict\"\n",
    "- name: \"MitieEntityExtractor\"\n",
    "- name: \"EntitySynonymMapper\"\n",
    "- name: \"RegexFeaturizer\"\n",
    "- name: \"MitieFeaturizer\"\n",
    "- name: \"SklearnIntentClassifier\"\n",
    "```\n",
    "其中 dictionary_path: \"data/jieba_userdict\"主要是为了结巴分词准确，相当于我们在用结巴分词之前添加自定义词典"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 根据我们的NLU设置的意图，槽位等信息构建domain文件\n",
    "```\n",
    "slots:\n",
    "  disease:\n",
    "    type: text\n",
    "  symptom:\n",
    "    type: text\n",
    "  dept:\n",
    "    type: text\n",
    "  drug:\n",
    "    type: text\n",
    "  food:\n",
    "    type: text\n",
    "  sure:\n",
    "    type: unfeaturized\n",
    "  pre_disease:\n",
    "    type: unfeaturized\n",
    "\n",
    "\n",
    "intents:\n",
    "  - first:\n",
    "      triggers: action_first\n",
    "  - greet\n",
    "  - bye\n",
    "  - affirmative\n",
    "#  - search_disease\n",
    "  - search_treat\n",
    "  - search_food\n",
    "  - search_symptom\n",
    "  - search_cause\n",
    "  - search_neopathy\n",
    "  - search_drug\n",
    "  - search_prevention\n",
    "  - search_drug_func\n",
    "  - search_disease_treat_time\n",
    "  - search_easy_get\n",
    "  - search_disease_dept\n",
    "\n",
    "entities:\n",
    "- disease\n",
    "- symptom\n",
    "- dept\n",
    "- drug\n",
    "- food\n",
    "\n",
    "templates:\n",
    "  utter_first:\n",
    "    - text: \"您好，我是您的医疗助手Friende，我是个机器人，请问有什么可以帮您？\"\n",
    "  utter_greet:\n",
    "    - text: \"您好～\"\n",
    "    - text: \"您好呀～\"\n",
    "  utter_goodbye:\n",
    "    - text: \"再见，祝您身体健康～\"\n",
    "    - text: \"拜拜，希望我有帮到您～\"\n",
    "  utter_howcanhelp:\n",
    "    - text: \"您可以这样向我提问: 头痛怎么办/\n",
    "                              什么人容易头痛/\n",
    "                              头痛吃什么药/\n",
    "                              头痛能治吗/\n",
    "                              头痛属于什么科/\n",
    "                              头孢地尼分散片用途/\n",
    "                              如何防止头痛/\n",
    "                              头痛要治多久/\n",
    "                              糖尿病有什么并发症/\n",
    "                              糖尿病有什么症状\"\n",
    "  utter_donknow:\n",
    "    - text: \"啊噢，我没有理解您说的话，我的理解力还需要更多的提升>_<。\"\n",
    "\n",
    "actions:\n",
    "- utter_first\n",
    "- utter_donknow\n",
    "- action_first\n",
    "- action_donknow\n",
    "- action_echo\n",
    "- action_search_treat\n",
    "- action_search_food\n",
    "- action_search_symptom\n",
    "- action_search_cause\n",
    "- action_search_neopathy\n",
    "- action_search_drug\n",
    "- action_search_prevention\n",
    "- action_search_drug_func\n",
    "- action_search_disease_treat_time\n",
    "- action_search_easy_get\n",
    "- action_search_disease_dept\n",
    "- utter_greet\n",
    "- utter_howcanhelp\n",
    "- utter_goodbye\n",
    "```\n",
    "**这里我们着重解释：templates部分**\n",
    "```\n",
    "  utter_goodbye:\n",
    "    - text: \"再见，祝您身体健康～\"\n",
    "    - text: \"拜拜，希望我有帮到您～\"\n",
    "```\n",
    "这里的一个模板名字为：utter_意图名字，也就是说只要按照这个格式命名了一个模板，后端action就会自动的去执行，下面的text就是回复的内容，如果存在多个text那么会随机选择一个回复。针对这个例子，只要我们在后端action设置，对于意图goodbye，我们在后端只需要dispatcher.utter_template(\"utter_goodbye\", tracker)，那么机器人就会从“再见，祝您身体健康～”，“拜拜，希望我有帮到您～”随机选择一个回复作为回复。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 如何在Windows 10下面运行？\n",
    "**安装MITIE**\n",
    "```\n",
    "请严格按照步骤安装Visual Studio、cmake、boost，并通过：pip install git+https://github.com/mit-nlp/MITIE.git 安装mitie\n",
    "安装指导：https://www.jianshu.com/p/063d7cd598d3\n",
    "```\n",
    "如果是Mac 或者Linux，无须额外安装上面三个插件"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 开始训练\n",
    "```\n",
    "rasa train\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### 作业\n",
    "* 精度DIET论文\n",
    "* 熟悉rasa各个NLU组件的特点以及使用场景\n",
    "* 完成NLU训练数据生成工具开发\n",
    "* 尝试用不同的rasa NLU组件做训练，并用Rasa的NLU测试工具做一些分析（有同学自己根据语料特点自定义NLU模块，用该工具测试NLU效果发了paper）\n",
    "* 使整个项目文件在自己的开发环境train顺利完成\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
